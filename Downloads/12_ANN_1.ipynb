{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e79ae388",
      "metadata": {
        "id": "e79ae388",
        "outputId": "954c8400-aa27-4232-ed38-3c0a15784208",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train_full, y_train_full),(x_test, y_test) = mnist.load_data()\n",
        "#The dataset is structured as a tuple of NumPy arrays: x training images, y training labels, and x test images, y test labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ac44742",
      "metadata": {
        "id": "6ac44742",
        "outputId": "8841d32a-585c-4a1f-aa4b-120bd2825351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbc278f5990>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(x_train_full[2])\n",
        "#It will show the 3rd training image as python indexing starts with 0.\n",
        "#perform data normalization so that all input values are between 0 and 1. \n",
        "#Since a grayscale image has pixel values of 0 to 255, we divide each pixel by 255. \n",
        "#Data normalization is necessary to reduce data redundancy and improve data integrity. \n",
        "#It makes sure your data is the same throughout your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a2ede1fe",
      "metadata": {
        "id": "a2ede1fe"
      },
      "outputs": [],
      "source": [
        "x_train_norm = x_train_full/255.                #dividing the entire dataset of 65k images by 255\n",
        "x_test_norm = x_test/255.                       #dividing the entire dataset of 65k images by 255\n",
        "\n",
        "x_valid, x_train = x_train_norm[:5000], x_train_norm[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "x_test = x_test_norm\n",
        "\n",
        "#Training data are fed after each training in an epoch. \n",
        "#An epoch is when the entire training dataset passes through the neural network once. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7b22e945",
      "metadata": {
        "id": "7b22e945"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "#42 is used as random seed in ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0871ae9b",
      "metadata": {
        "id": "0871ae9b"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "# 1 flatten layer as the input layer\n",
        "# 2 dense relu layers as hidden layers \n",
        "# a dense softmax layer as the output layer\n",
        "\n",
        "#A flattening layer flattens the input to a single-column array. It prepares the input data for the next dense layers. \n",
        "#a dense layer is a layer of parallel perceptrons.\n",
        "\n",
        "# relu is used on hidden layers\n",
        "#sigmoid and softmax are used for output. \n",
        "#Softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0a8bc412",
      "metadata": {
        "id": "0a8bc412",
        "outputId": "a90d28db-33b8-41b5-cce7-8a75144f5e36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 300)               235500    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               30100     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()\n",
        "#function shows the number of parameters in each layer, how the output shape changes, \n",
        "#and the total amount of parameters to be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "de30da1a",
      "metadata": {
        "id": "de30da1a"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "#sgd means stochastic gradient descent – a type of gradient descent that only uses a single training example per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "090de173",
      "metadata": {
        "id": "090de173",
        "outputId": "3a7cb264-3d3a-41a8-a4d7-6155f22db57c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6197 - accuracy: 0.8405 - val_loss: 0.3103 - val_accuracy: 0.9120\n",
            "Epoch 2/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2953 - accuracy: 0.9151 - val_loss: 0.2473 - val_accuracy: 0.9302\n",
            "Epoch 3/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2429 - accuracy: 0.9304 - val_loss: 0.2060 - val_accuracy: 0.9424\n",
            "Epoch 4/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2074 - accuracy: 0.9406 - val_loss: 0.1823 - val_accuracy: 0.9494\n",
            "Epoch 5/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1810 - accuracy: 0.9487 - val_loss: 0.1644 - val_accuracy: 0.9540\n",
            "Epoch 6/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1603 - accuracy: 0.9544 - val_loss: 0.1482 - val_accuracy: 0.9600\n",
            "Epoch 7/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1431 - accuracy: 0.9597 - val_loss: 0.1366 - val_accuracy: 0.9632\n",
            "Epoch 8/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1292 - accuracy: 0.9631 - val_loss: 0.1295 - val_accuracy: 0.9638\n",
            "Epoch 9/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1175 - accuracy: 0.9672 - val_loss: 0.1184 - val_accuracy: 0.9672\n",
            "Epoch 10/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1078 - accuracy: 0.9696 - val_loss: 0.1091 - val_accuracy: 0.9688\n",
            "Epoch 11/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0988 - accuracy: 0.9719 - val_loss: 0.1049 - val_accuracy: 0.9694\n",
            "Epoch 12/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0912 - accuracy: 0.9745 - val_loss: 0.1005 - val_accuracy: 0.9706\n",
            "Epoch 13/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0844 - accuracy: 0.9763 - val_loss: 0.0948 - val_accuracy: 0.9730\n",
            "Epoch 14/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0782 - accuracy: 0.9779 - val_loss: 0.0921 - val_accuracy: 0.9736\n",
            "Epoch 15/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0727 - accuracy: 0.9794 - val_loss: 0.0876 - val_accuracy: 0.9730\n",
            "Epoch 16/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0678 - accuracy: 0.9805 - val_loss: 0.0860 - val_accuracy: 0.9740\n",
            "Epoch 17/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0633 - accuracy: 0.9823 - val_loss: 0.0833 - val_accuracy: 0.9752\n",
            "Epoch 18/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0593 - accuracy: 0.9835 - val_loss: 0.0798 - val_accuracy: 0.9764\n",
            "Epoch 19/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0556 - accuracy: 0.9850 - val_loss: 0.0781 - val_accuracy: 0.9772\n",
            "Epoch 20/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0520 - accuracy: 0.9860 - val_loss: 0.0773 - val_accuracy: 0.9762\n",
            "Epoch 21/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0489 - accuracy: 0.9868 - val_loss: 0.0735 - val_accuracy: 0.9784\n",
            "Epoch 22/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0459 - accuracy: 0.9877 - val_loss: 0.0741 - val_accuracy: 0.9780\n",
            "Epoch 23/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0432 - accuracy: 0.9884 - val_loss: 0.0755 - val_accuracy: 0.9772\n",
            "Epoch 24/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0405 - accuracy: 0.9893 - val_loss: 0.0726 - val_accuracy: 0.9774\n",
            "Epoch 25/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0384 - accuracy: 0.9901 - val_loss: 0.0705 - val_accuracy: 0.9792\n",
            "Epoch 26/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0361 - accuracy: 0.9910 - val_loss: 0.0678 - val_accuracy: 0.9798\n",
            "Epoch 27/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0341 - accuracy: 0.9916 - val_loss: 0.0671 - val_accuracy: 0.9806\n",
            "Epoch 28/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0322 - accuracy: 0.9922 - val_loss: 0.0690 - val_accuracy: 0.9804\n",
            "Epoch 29/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0301 - accuracy: 0.9932 - val_loss: 0.0674 - val_accuracy: 0.9792\n",
            "Epoch 30/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0286 - accuracy: 0.9935 - val_loss: 0.0671 - val_accuracy: 0.9800\n"
          ]
        }
      ],
      "source": [
        "model_history = model.fit(x_train,y_train,epochs=30,validation_data=(x_valid,y_valid))\n",
        "#accuracy = 99% using our test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cd200b1c",
      "metadata": {
        "id": "cd200b1c"
      },
      "outputs": [],
      "source": [
        "x_sample = x_test[:5]\n",
        "y_probability = model.predict(x_sample)\n",
        "#took 5 images – index 0 to 4, and feed them to model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ac6325ab",
      "metadata": {
        "id": "ac6325ab",
        "outputId": "5cec11ad-b2c1-4c19-f6fa-a184523c0237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "y_probability.round()\n",
        "#returns the probability of the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9c454df8",
      "metadata": {
        "id": "9c454df8"
      },
      "outputs": [],
      "source": [
        "y_predict = model.predict(x_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d0ed62",
      "metadata": {
        "id": "f2d0ed62"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "push on github 12 ANN_1.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}